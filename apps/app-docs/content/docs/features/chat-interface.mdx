---
title: Chat Interface
description: AI-powered chat with your notes using RAG
---

## Overview

The chat interface allows users to have intelligent conversations with their processed notes. It uses Retrieval-Augmented Generation (RAG) to provide contextually relevant responses based on the note's content.

## How It Works

### RAG Pipeline

1. **User Query**: User asks a question about their note
2. **Vector Search**: Query is embedded and used to find relevant chunks
3. **Context Retrieval**: Top matching chunks are retrieved
4. **LLM Generation**: Retrieved context is passed to the language model
5. **Response**: AI generates a response based on the context

## Chat Implementation

### Server Action

The `chatWithNote` action handles the RAG pipeline:

```typescript
// apps/app/actions/chat-with-note.ts
export async function chatWithNote(noteId: string, message: string) {
  // 1. Generate embedding for the query
  const queryEmbedding = await generateEmbedding(message);

  // 2. Find relevant chunks using vector similarity
  const relevantChunks = await prisma.$queryRaw`
    SELECT content, embedding <=> ${queryEmbedding}::vector as distance
    FROM "NoteChunk"
    WHERE "noteId" = ${noteId}
    ORDER BY distance
    LIMIT 5
  `;

  // 3. Construct context from relevant chunks
  const context = relevantChunks
    .map((chunk) => chunk.content)
    .join("\n\n");

  // 4. Generate response with LLM
  const response = await generateResponse(message, context);

  return response;
}
```

### Vector Similarity Search

The application uses PostgreSQL's pgvector extension for similarity search:

```sql
-- Find chunks similar to a query vector
SELECT
  content,
  embedding <=> $1::vector as distance
FROM "NoteChunk"
WHERE "noteId" = $2
ORDER BY distance
LIMIT 5
```

**Distance Operators**:
- `<=>`: Cosine distance (used in this app)
- `<->`: Euclidean distance
- `<#>`: Inner product

## Chat Component

### NoteChatbot

The main chat interface component:

```tsx
// apps/app/app/(protected)/ai-text-summarizer/[id]/components/note-chatbot.tsx
export function NoteChatbot({ noteId }: { noteId: string }) {
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState("");
  const [isLoading, setIsLoading] = useState(false);

  const handleSubmit = async () => {
    setIsLoading(true);

    // Add user message
    const userMessage = { role: "user", content: input };
    setMessages((prev) => [...prev, userMessage]);

    // Get AI response
    const response = await chatWithNote(noteId, input);

    // Add AI message
    const aiMessage = { role: "assistant", content: response };
    setMessages((prev) => [...prev, aiMessage]);

    setInput("");
    setIsLoading(false);
  };

  return (
    <div className="flex flex-col h-full">
      <div className="flex-1 overflow-y-auto">
        {messages.map((message, i) => (
          <ChatMessage key={i} message={message} />
        ))}
      </div>
      <ChatInput
        value={input}
        onChange={setInput}
        onSubmit={handleSubmit}
        isLoading={isLoading}
      />
    </div>
  );
}
```

## Streaming Responses

For better UX, implement streaming responses:

```typescript
import { streamText } from "ai";
import { openai } from "@ai-sdk/openai";

export async function chatWithNote(noteId: string, message: string) {
  const relevantChunks = await getRelevantChunks(noteId, message);
  const context = relevantChunks.map((c) => c.content).join("\n\n");

  const stream = await streamText({
    model: openai("gpt-4"),
    messages: [
      {
        role: "system",
        content: `You are a helpful assistant. Answer questions based on this context:\n\n${context}`,
      },
      {
        role: "user",
        content: message,
      },
    ],
  });

  return stream.toDataStreamResponse();
}
```

## Context Window Management

### Limiting Retrieved Chunks

Balance between context quality and token limits:

```typescript
const MAX_CHUNKS = 5;
const MAX_CHUNK_LENGTH = 500;

const relevantChunks = await prisma.$queryRaw`
  SELECT content, embedding <=> ${queryEmbedding}::vector as distance
  FROM "NoteChunk"
  WHERE "noteId" = ${noteId}
  ORDER BY distance
  LIMIT ${MAX_CHUNKS}
`;

// Truncate chunks if needed
const context = relevantChunks
  .map((chunk) => chunk.content.slice(0, MAX_CHUNK_LENGTH))
  .join("\n\n");
```

## Advanced Features

### Chat History

Store conversation history for context:

```prisma
model ChatMessage {
  id        String   @id @default(cuid())
  noteId    String
  role      String   // "user" or "assistant"
  content   String   @db.Text
  createdAt DateTime @default(now())
  note      Note     @relation(fields: [noteId], references: [id])

  @@index([noteId])
}
```

### Metadata Filtering

Add metadata to chunks for better filtering:

```prisma
model NoteChunk {
  id        String   @id @default(cuid())
  noteId    String
  content   String   @db.Text
  embedding Unsupported("vector(1536)")?
  position  Int
  metadata  Json?    // Store section, page number, etc.
  note      Note     @relation(fields: [noteId], references: [id])
}
```

### Multi-turn Conversations

Maintain conversation context across turns:

```typescript
export async function chatWithNote(
  noteId: string,
  message: string,
  history: Message[]
) {
  const relevantChunks = await getRelevantChunks(noteId, message);
  const context = formatContext(relevantChunks);

  const response = await streamText({
    model: openai("gpt-4"),
    messages: [
      {
        role: "system",
        content: `Context: ${context}`,
      },
      ...history, // Include previous messages
      {
        role: "user",
        content: message,
      },
    ],
  });

  return response;
}
```

## Best Practices

- **Chunk Limit**: Start with 3-5 chunks; adjust based on results
- **Reranking**: Consider adding a reranking step for better relevance
- **Caching**: Cache embeddings and frequently accessed chunks
- **Error Handling**: Gracefully handle API failures and timeouts
- **User Feedback**: Allow users to rate responses for improvement
